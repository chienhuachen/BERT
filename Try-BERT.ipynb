{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample of BertModel\n",
    "\n",
    "#!pip install transformers\n",
    "\n",
    "#install pytorch\n",
    "#https://pytorch.org/get-started/locally/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "BERT_PRETRAINED_MODEL_ARCHIVE_LIST = [\n",
    "    \"bert-base-uncased\",\n",
    "    \"bert-large-uncased\",\n",
    "    \"bert-base-cased\",\n",
    "    \"bert-large-cased\",\n",
    "    \"bert-base-multilingual-uncased\",\n",
    "    \"bert-base-multilingual-cased\",\n",
    "    \"bert-base-chinese\",\n",
    "    # See all BERT models at https://huggingface.co/models?filter=bert\n",
    "]\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "Corpus = [\n",
    "    \"I am not smart but not a fool.\",\n",
    "    \"She is very pretty but not my girl friend.\"\n",
    "]\n",
    "\n",
    "inputs = tokenizer(Corpus, return_tensors=\"pt\",\n",
    "                    max_length=15, padding=True, truncation=True)\n",
    "\n",
    "input_ids = inputs['input_ids']\n",
    "token_type_ids = inputs['token_type_ids']\n",
    "attention_mask = inputs['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 101, 1045, 2572, 2025, 6047, 2021, 2025, 1037, 7966, 1012,  102,    0],\n",
       "        [ 101, 2016, 2003, 2200, 3492, 2021, 2025, 2026, 2611, 2767, 1012,  102]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_type_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary sizeï¼š 30522\n"
     ]
    }
   ],
   "source": [
    "vocab = tokenizer.vocab\n",
    "print(\"Dictionary sizeï¼š\", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1045, 2572, 1012, 0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab['i'], vocab['am'], vocab['.'], vocab['[PAD]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('[CLS]', 'i', 'am', '...', 'a', 'fool', '.', '[SEP]', '[PAD]')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_word = {index:word for word, index in vocab.items()}\n",
    "\n",
    "index_word[101], index_word[1045], index_word[2572], \"...\", \\\n",
    "index_word[1037], index_word[7966], index_word[1012], index_word[102], index_word[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('[CLS]', 'she', 'is', '...', 'my', 'girl', 'friend', '.', '[SEP]')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_word[101], index_word[2016], index_word[2003], \"...\", \\\n",
    "index_word[2026], index_word[2611], index_word[2767], index_word[1012], index_word[102]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPooling(last_hidden_state=tensor([[[-0.1046,  0.1310, -0.0827,  ..., -0.0219,  0.2136,  0.2302],\n",
       "         [-0.5176,  0.1067, -0.2878,  ..., -0.6373,  0.3295,  0.0065],\n",
       "         [-0.4615,  0.0065, -0.4400,  ..., -0.8397,  0.2282,  0.0466],\n",
       "         ...,\n",
       "         [ 0.6535,  0.1053, -0.4081,  ...,  0.0280, -0.3853, -0.3480],\n",
       "         [ 0.5521,  0.0689, -0.2095,  ...,  0.0432, -0.4177, -0.3825],\n",
       "         [ 0.1398,  0.1865,  0.3135,  ...,  0.2208,  0.1608, -0.0346]],\n",
       "\n",
       "        [[-0.2987, -0.3447, -0.4963,  ..., -0.1937,  0.3750,  0.5720],\n",
       "         [-0.2361, -0.9089, -0.0500,  ..., -0.4122,  0.2629,  0.2248],\n",
       "         [-0.0292, -0.6746, -0.2885,  ..., -0.7879,  0.0873,  0.5768],\n",
       "         ...,\n",
       "         [-0.5918, -1.5879,  0.3443,  ...,  0.6541, -0.0065,  0.4885],\n",
       "         [ 0.5235, -0.1575, -0.3188,  ..., -0.0518, -0.2579, -0.4995],\n",
       "         [ 0.4285, -0.1204, -0.0739,  ..., -0.0077, -0.3160, -0.5173]]],\n",
       "       grad_fn=<NativeLayerNormBackward>), pooler_output=tensor([[-0.6052, -0.1877,  0.5065,  ...,  0.5284, -0.4884,  0.7288],\n",
       "        [-0.4933, -0.3037, -0.4900,  ..., -0.1944, -0.5672,  0.8081]],\n",
       "       grad_fn=<TanhBackward>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# return_dict = True\n",
    "\n",
    "model = BertModel.from_pretrained('bert-base-uncased', return_dict=True)\n",
    "\n",
    "outputs = model(input_ids, token_type_ids, attention_mask)\n",
    "\n",
    "outputs    # type(outputs) = transformers.modeling_outputs.BaseModelOutputWithPooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6052, -0.1877,  0.5065,  ...,  0.5284, -0.4884,  0.7288],\n",
       "        [-0.4933, -0.3037, -0.4900,  ..., -0.1944, -0.5672,  0.8081]],\n",
       "       grad_fn=<TanhBackward>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_output = outputs.last_hidden_state\n",
    "pooler_output = outputs.pooler_output\n",
    "hidden_states = outputs.hidden_states\n",
    "attentions = outputs.attentions\n",
    "\n",
    "pooler_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6052, -0.1877,  0.5065,  ...,  0.5284, -0.4884,  0.7288],\n",
       "        [-0.4933, -0.3037, -0.4900,  ..., -0.1944, -0.5672,  0.8081]],\n",
       "       grad_fn=<TanhBackward>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# return_dict = False\n",
    "\n",
    "model = BertModel.from_pretrained('bert-base-uncased', return_dict=False)\n",
    "\n",
    "outputs = model(input_ids, token_type_ids, attention_mask)\n",
    "\n",
    "sequence_output, pooled_output = outputs   # type(outputs) = tuple\n",
    "\n",
    "pooled_output"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "https://huggingface.co/transformers/model_doc/bert.html\n",
    "https://huggingface.co/transformers/task_summary.html\n",
    "\n",
    "BertForPreTraining\n",
    "BertModelLMHeadModel\n",
    "BertForMaskedLM\n",
    "BertForNextSentencePrediction\n",
    "BertForSequenceClassification\n",
    "BertForMultipleChoice\n",
    "BertForTokenClassification\n",
    "BertForQuestionAnswering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#Bert For Sequence Classification\n",
    "#https://huggingface.co/transformers/model_doc/bert.html#bertforsequenceclassification\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', return_dict=True, num_labels=2)\n",
    "\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n",
    "\n",
    "outputs = model(**inputs, labels=labels)\n",
    "\n",
    "loss = outputs.loss\n",
    "logits = outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2437, -0.8184],\n",
      "        [ 0.0301, -0.8116]], grad_fn=<AddmmBackward>) tensor(0.8587, grad_fn=<NllLossBackward>)\n",
      "tensor([[-0.1394, -0.6609],\n",
      "        [-0.1729, -0.4143]], grad_fn=<AddmmBackward>) tensor(0.7836, grad_fn=<NllLossBackward>)\n",
      "tensor([[ 0.0328, -0.3256],\n",
      "        [-0.0854, -0.6762]], grad_fn=<AddmmBackward>) tensor(0.6645, grad_fn=<NllLossBackward>)\n",
      "tensor([[-0.0608, -0.4446],\n",
      "        [-0.2107, -0.6144]], grad_fn=<AddmmBackward>) tensor(0.7074, grad_fn=<NllLossBackward>)\n",
      "tensor([[-0.3810, -0.1820],\n",
      "        [-0.0318, -0.6771]], grad_fn=<AddmmBackward>) tensor(0.5101, grad_fn=<NllLossBackward>)\n",
      "tensor([[-0.2172, -0.1352],\n",
      "        [-0.0507, -0.6385]], grad_fn=<AddmmBackward>) tensor(0.5474, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "EPOCHS = 6  \n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    inputs = tokenizer([\"Hello, my dog is cute\",\n",
    "                        \"Sorry, your key is lost\"], return_tensors=\"pt\")\n",
    "    labels = torch.tensor([1, \n",
    "                           0]).unsqueeze(0)  # Batch size 1\n",
    "    \n",
    "    # forward\n",
    "    outputs = model(**inputs, labels=labels)\n",
    "\n",
    "    loss = outputs.loss # the model will return if labels were given.\n",
    "    logits = outputs.logits\n",
    "\n",
    "    # backward\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(logits, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom pytorch model \n",
    "class Bert_CNN_Tabular_Classification(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Bert_CNN_Tabular_Classification, self).__init__()\n",
    "        \n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        bert_out_features = 768\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        cnn_out_features = 500\n",
    "        self.cnn = models.resnet50(pretrained=True)\n",
    "        for param in self.cnn.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.cnn.fc = nn.Linear(self.cnn.fc.in_features, cnn_out_features)        \n",
    "        \n",
    "        tabular_features = 381\n",
    "        self.classifier = nn.Linear(bert_out_features + cnn_out_features + tabular_features, 5)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_tabular=None,\n",
    "        input_image=None,        \n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None):\n",
    "        \n",
    "        sequence_output, pooled_output = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "\n",
    "        text_output = self.dropout(pooled_output)\n",
    "        \n",
    "        image_output = self.cnn(input_image)\n",
    "    \n",
    "        total = torch.cat([text_output, image_output, input_tabular.float()], axis = 1)\n",
    "        \n",
    "        logits = self.classifier(total)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How many pretrained models are available in Transformers?\n",
      "Answer: over 32 +\n",
      "\n",
      "Question: What does Transformers provide?\n",
      "Answer: general - purpose architectures\n",
      "\n",
      "Question: Transformers provides interoperability between which frameworks?\n",
      "Answer: tensorflow 2 . 0 and pytorch\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# AutoModelForQuestionAnswering\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
    "\n",
    "text = r\"\"\"\n",
    "ðŸ¤— Transformers (formerly known as pytorch-transformers and pytorch-pretrained-bert) provides general-purpose\n",
    "architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNetâ€¦) for Natural Language Understanding (NLU) and Natural\n",
    "Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between\n",
    "TensorFlow 2.0 and PyTorch.\n",
    "\"\"\"\n",
    "\n",
    "questions = [\n",
    "    \"How many pretrained models are available in Transformers?\",\n",
    "    \"What does Transformers provide?\",\n",
    "    \"Transformers provides interoperability between which frameworks?\",\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    inputs = tokenizer.encode_plus(question, text, add_special_tokens=True, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].tolist()[0]\n",
    "\n",
    "    text_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    answer_start_scores, answer_end_scores = model(**inputs)\n",
    "\n",
    "    answer_start = torch.argmax(\n",
    "        answer_start_scores\n",
    "    )  # Get the most likely beginning of answer with the argmax of the score\n",
    "    answer_end = torch.argmax(answer_end_scores) + 1  # Get the most likely end of answer with the argmax of the score\n",
    "\n",
    "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
    "\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {answer}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Which name is also used to describe the Amazon rainforest in English?\n",
      "Answer: amazonia\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-distilled-squad\")\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"distilbert-base-uncased-distilled-squad\")\n",
    "\n",
    "text = r\"\"\"\n",
    "The Amazon rainforest (Portuguese: Floresta AmazÃ´nica or AmazÃ´nia; Spanish: Selva AmazÃ³nica, AmazonÃ­a or usually Amazonia; \n",
    "French: ForÃªt amazonienne; Dutch: Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is a moist \n",
    "broadleaf forest that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 square kilometres\n",
    "(2,700,000 sq mi), of which 5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This region includes\n",
    "territory belonging to nine nations. The majority of the forest is contained within Brazil, with 60% of the rainforest, \n",
    "followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia, Guyana, Suriname and \n",
    "French Guiana. States or departments in four nations contain \"Amazonas\" in their names. The Amazon represents over half of \n",
    "the planet's remaining rainforests, and comprises the largest and most biodiverse tract of tropical rainforest in the world,\n",
    "with an estimated 390 billion individual trees divided into 16,000 species\n",
    "\"\"\"\n",
    "\n",
    "questions = [\n",
    "    \"Which name is also used to describe the Amazon rainforest in English?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    inputs = tokenizer.encode_plus(question, text, add_special_tokens=True, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].tolist()[0]\n",
    "\n",
    "    text_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    answer_start_scores, answer_end_scores = model(**inputs)\n",
    "\n",
    "    answer_start = torch.argmax(\n",
    "        answer_start_scores\n",
    "    )  # Get the most likely beginning of answer with the argmax of the score\n",
    "    answer_end = torch.argmax(answer_end_scores) + 1  # Get the most likely end of answer with the argmax of the score\n",
    "\n",
    "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
    "\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {answer}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9978193640708923}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Allocate a pipeline for sentiment-analysis\n",
    "classifier = pipeline('sentiment-analysis')\n",
    "\n",
    "classifier('We are very happy to include pipeline into the transformers repository.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.5135967135429382,\n",
       " 'start': 35,\n",
       " 'end': 59,\n",
       " 'answer': 'huggingface/transformers'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Allocate a pipeline for question-answering\n",
    "question_answerer = pipeline('question-answering')\n",
    "\n",
    "question_answerer({\n",
    "    'question': 'What is the name of the repository ?',\n",
    "    'context': 'Pipeline have been included in the huggingface/transformers repository'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'translation_text': 'Hugging Face ist ein Technologieunternehmen mit Sitz in New York und Paris.'}]\n"
     ]
    }
   ],
   "source": [
    "#https://pytorch.org/hub/pytorch_fairseq_translation/\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "translator = pipeline(\"translation_en_to_de\")\n",
    "\n",
    "print(translator(\"Hugging Face is a technology company based in New York and Paris\", max_length=40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
